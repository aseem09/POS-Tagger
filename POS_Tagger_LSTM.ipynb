{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Sbq6xDXIj0T"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "import multiprocessing\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import backend as K\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm9MeicGIj0U"
   },
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    \n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for s_tag in root.iter('s'):\n",
    "        \n",
    "        sentence = []\n",
    "        tags = []\n",
    "        \n",
    "        for e_tag in s_tag:\n",
    "                \n",
    "            if e_tag.tag == 'w' or e_tag.tag == 'c':\n",
    "                if e_tag.text is not None:\n",
    "                    tag = e_tag.attrib['c5']\n",
    "                    word = e_tag.text.replace(\" \", \"\")\n",
    "                    \n",
    "                    sentence.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "            elif e_tag.tag == 'mw':\n",
    "                tag = e_tag.attrib['c5']\n",
    "                word = \"\"\n",
    "                for w_tag in e_tag.iterfind('w'):     \n",
    "                    word += w_tag.text.replace(\" \", \"\")\n",
    "                \n",
    "                sentence.append(word)\n",
    "                tags.append(tag)\n",
    "                \n",
    "            elif e_tag.tag == 'hi' or e_tag.tag == 'corr':\n",
    "                \n",
    "                for r_tag in e_tag:\n",
    "                \n",
    "                    if r_tag.tag == 'w' or r_tag.tag == 'c':\n",
    "                        if r_tag.text is not None:\n",
    "                            tag = r_tag.attrib['c5']\n",
    "                            word = r_tag.text.replace(\" \", \"\")\n",
    "\n",
    "                            sentence.append(word)\n",
    "                            tags.append(tag)\n",
    "\n",
    "                    elif r_tag.tag == 'mw':\n",
    "                        tag = r_tag.attrib['c5']\n",
    "                        word = \"\"\n",
    "                        for w_tag in r_tag.iterfind('w'):     \n",
    "                            word += w_tag.text.replace(\" \", \"\")\n",
    "\n",
    "                        sentence.append(word)\n",
    "                        tags.append(tag)\n",
    "                \n",
    "        data.append(sentence)\n",
    "        labels.append(tags)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QpNALZSIj0U"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "\n",
    "            fileName = subdir + '/' + str(file)\n",
    "            file_data, file_labels = parse_data(fileName)\n",
    "            data.extend(file_data)\n",
    "            labels.extend(file_labels)\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfD563pVIj0U"
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "train_path = 'Train-corpus/'\n",
    "test_path = 'Test-corpus/'\n",
    "\n",
    "data, labels = load_dataset(train_path)\n",
    "test_data, test_labels = load_dataset(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utQnqd6EIj0U",
    "outputId": "4d0faee0-87c6-44c8-fc9e-ab8ef8803687",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "print(data[0])\n",
    "print(labels[0])\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(test_labels))\n",
    "\n",
    "print(test_data[11])\n",
    "print(test_labels[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_cap(cap, data, labels):\n",
    "    d = []\n",
    "    l = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        if len(sentence) < cap:\n",
    "            d.append(sentence)\n",
    "            l.append(labels[i])\n",
    "            \n",
    "    return d, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data\n",
    "b = labels\n",
    "c = test_data\n",
    "d = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = len_cap(32, a, b)\n",
    "test_data, test_labels = len_cap(32, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(len(labels))\n",
    "print(len(test_data))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in data:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "        \n",
    "for ts in labels:\n",
    "    for t in ts:\n",
    "        tags.add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical():\n",
    "    train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    "    \n",
    "    for s in data:\n",
    "        s_int = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                s_int.append(word2index[w.lower()])\n",
    "            except KeyError:\n",
    "                s_int.append(word2index['-OOV-'])\n",
    "\n",
    "        train_sentences_X.append(s_int)\n",
    "\n",
    "    for s in test_data:\n",
    "        s_int = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                s_int.append(word2index[w.lower()])\n",
    "            except KeyError:\n",
    "                s_int.append(word2index['-OOV-'])\n",
    "\n",
    "        test_sentences_X.append(s_int)\n",
    "\n",
    "    for s in labels:\n",
    "        train_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "    for s in test_labels:\n",
    "        test_tags_y.append([tag2index[t] for t in s])\n",
    "    \n",
    "    return train_sentences_X, train_tags_y, test_sentences_X, test_tags_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int, labels_int, test_data_int, test_labels_int = convert_to_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_int[0])\n",
    "print(labels_int[0])\n",
    "print(test_data_int[0])\n",
    "print(test_labels_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = len(max(data, key=len))\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int = pad_sequences(data_int, maxlen=MAX_LENGTH, padding='post')\n",
    "labels_int = pad_sequences(labels_int, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_int = pad_sequences(test_data_int, maxlen=MAX_LENGTH, padding='post')\n",
    "test_labels_int = pad_sequences(test_labels_int, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(data)\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "encoded_docs_test = t.texts_to_sequences(test_data)\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(padded_docs[0])\n",
    "print(data_int[0])\n",
    "print(type(padded_docs))\n",
    "print(type(data_int))\n",
    "print(type(padded_docs_test.tolist()))\n",
    "print(type(test_data_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "#             print(item)\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = to_categorical(labels_int, len(tag2index))\n",
    "# print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "#     model.add(layers.Embedding(len(word2index), 128))\n",
    "    model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False))\n",
    "    \n",
    "    model.add(layers.Bidirectional(layers.LSTM(256, return_sequences=True)))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(len(tag2index))))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(padded_docs, one_hot_labels, batch_size=4096, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels_test = to_categorical(test_labels_int, len(tag2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(one_hot_labels[0]))\n",
    "print(type(one_hot_labels_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(padded_docs[0][0]))\n",
    "print(type(padded_docs_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(padded_docs_test, one_hot_labels_test, batch_size=4096)\n",
    "print(f\"{model.metrics_names[2]}: {scores[2] * 100}\")\n",
    "# model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data_int)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(test_data, test_labels, preds):\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    print(\"Total: %d\" % len(test_data))\n",
    "    \n",
    "    t0 = time.process_time()\n",
    "    \n",
    "    for index, pred_labels in enumerate(preds):\n",
    "        true_labels = test_labels[index]\n",
    "        \n",
    "        for i, pred_label in enumerate(pred_labels):\n",
    "            if pred_label in true_labels[i]:\n",
    "                correct = correct + 1\n",
    "            else:\n",
    "                incorrect = incorrect + 1\n",
    "                \n",
    "    print(\"Evaluated Words: %d \" % (incorrect + correct))   \n",
    "    print(\"Correct: %d \" % (correct))   \n",
    "    print(\"Incorrect: %d \" % (incorrect))   \n",
    "    print(\"Time Taken: %.2f \\n \" % (time.process_time()-t0))\n",
    "    \n",
    "    print(\"Final Accuracy = %.06f\"  % (correct/(correct+incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkES1f_ROwPn"
   },
   "outputs": [],
   "source": [
    "# Load JSON Files\n",
    "\n",
    "with open('words.json') as f:\n",
    "    word_dict = json.load(f)\n",
    "with open('tags.json') as f:\n",
    "    tag_dict = json.load(f)\n",
    "with open('word_tags.json') as f:\n",
    "    word_tags_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6XVaCmZOwPs",
    "outputId": "9c487106-7292-462d-affc-12c5ae84266d"
   },
   "outputs": [],
   "source": [
    "print(len(tag_dict))\n",
    "print(len(word_dict))\n",
    "print(len(word_tags_dict))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POS-Tagger-HMM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
